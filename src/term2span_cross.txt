ssh://fuyj@115.24.15.21:22/home/fuyj/workspace/venv/torch/bin/python3 -u /home/fuyj/.pycharm_helpers/pydev/pydevd.py --multiproc --qt-support=auto --client 0.0.0.0 --port 37321 --file /home/fuyj/workspace/experiment/ABAM-main/src/run_AURC_token_term2span.py
pydev debugger: process 2147 is connecting

Connected to pydev debugger (build 193.7288.30)
device cuda:0
../models_final/aurc_cr_token.pt
../models_final/aurc_cr_config.json
../models_final/aurc_cr_predictions_dev.json
../models_final/aurc_cr_predictions_test.json
../data/../data/data_dict_bert_pieces2word.json
数据集大小
4396
8 ['abortion', 'cloning', 'death penalty', 'gun control', 'marijuana legalization', 'minimum wage', 'nuclear energy', 'school uniforms']
{'abortion': 415, 'death penalty': 588, 'gun control': 480, 'marijuana legalization': 626, 'minimum wage': 624, 'nuclear energy': 615, 'school uniforms': 705, 'cloning': 343}
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/home/fuyj/workspace/venv/torch/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
0
/home/fuyj/workspace/experiment/ABAM-main/src/run_AURC_token_term2span.py:342: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)
  all_pieces2word = torch.tensor([f.pieces2word for f in train_features])
2097 66
478 478
1185 1185
Some weights of the model checkpoint at ../model_base/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2600
##### DOMAIN: cross , use CRF: True , learning-rate: 1e-05 , DROPOUT: 0.1

Epoch:    0 2022-09-03 22:22:46.974899
/home/fuyj/workspace/venv/torch/lib/python3.6/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:328.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
[[11235  1708     0  3762     0]
 [ 2714  6495     0  3848     0]
 [  229  1790     0  2081     2]
 [ 3421  2273     0 11084     0]
 [  237   513     0  4862     0]]
TRAIN:  Pre. 0.314 | Rec. 0.366 | F1 0.335
[[3327  327    0  373    0]
 [2137 1827    0 1537    0]
 [ 173  464    0  638    0]
 [ 610  667    0  534    0]
 [  49  159    0  194    0]]
EVAL:   Pre. 0.244 | Rec. 0.291 | F1 0.253 | BEST F1: 0.000 None
[[6611  955    0 1242    0]
 [2428 3511    0 1992    0]
 [ 223  758    0 1053    0]
 [2363 2939    0 3862    0]
 [ 174  686    0 1624    0]]
TEST:   Pre. 0.270 | Rec. 0.323 | F1 0.294

Epoch:    1 2022-09-03 22:24:50.904143
[[ 9463  2743   106  4103   290]
 [  746  9477   235  2477   122]
 [   65  2256   757   626   398]
 [  762  2853    48 12678   437]
 [   60   555   257  2661  2079]]
TRAIN:  Pre. 0.622 | Rec. 0.521 | F1 0.536
[[2815  657    8  539    8]
 [ 711 2864   31 1858   37]
 [  61  647   74  425   68]
 [ 174  915    3  707   12]
 [  16  190   16  159   21]]
EVAL:   Pre. 0.437 | Rec. 0.344 | F1 0.338 | BEST F1: 0.253 0
[[5367 1457   22 1913   49]
 [ 778 4683   27 2372   71]
 [  57 1021  135  685  136]
 [ 560 3439   27 5056   82]
 [  36  769  112 1309  258]]
TEST:   Pre. 0.500 | Rec. 0.384 | F1 0.390

Epoch:    2 2022-09-03 22:27:00.796156
[[ 9983  2111   229  3795   587]
 [  545  9648   559  2104   201]
 [   46  1345  1852   251   608]
 [  562  1636    75 13547   958]
 [   46   190   294  1502  3580]]
TRAIN:  Pre. 0.679 | Rec. 0.647 | F1 0.652
[[2732  484   31  706   74]
 [ 551 2253  128 2343  226]
 [  46  349  233  349  298]
 [ 128  652   27  931   73]
 [  15  100   36  147  104]]
EVAL:   Pre. 0.446 | Rec. 0.409 | F1 0.391 | BEST F1: 0.338 1
[[5318 1225   83 1966  216]
 [ 607 4254  126 2734  210]
 [  48  694  388  479  425]
 [ 462 2906   96 5422  278]
 [  32  497  262  911  782]]
TEST:   Pre. 0.511 | Rec. 0.447 | F1 0.464

Epoch:    3 2022-09-03 22:29:04.139108
[[10520  1647   259  3560   719]
 [  433  9459   756  2132   277]
 [   49   733  2395   138   787]
 [  402   689    45 14293  1349]
 [   37    44   135   881  4515]]
TRAIN:  Pre. 0.722 | Rec. 0.719 | F1 0.709
[[2691  352   41  785  158]
 [ 499 1695  166 2707  434]
 [  43  177  230  246  579]
 [ 111  486   38 1011  165]
 [  16   59   47   86  194]]
EVAL:   Pre. 0.438 | Rec. 0.440 | F1 0.380 | BEST F1: 0.391 2

Epoch:    4 2022-09-03 22:30:25.415321
[[11902   956   198  3062   587]
 [  692  9425   704  2030   206]
 [   84   680  2493   129   716]
 [  476   265    21 14875  1141]
 [   37    24    57   925  4569]]
TRAIN:  Pre. 0.758 | Rec. 0.749 | F1 0.745
[[2890  211   29  736  161]
 [ 664 1284  132 2990  431]
 [  61  130  214  262  608]
 [ 154  330   20 1136  171]
 [  16   44   33   92  217]]
EVAL:   Pre. 0.452 | Rec. 0.457 | F1 0.375 | BEST F1: 0.391 2

Epoch:    5 2022-09-03 22:31:40.335503
[[12619  1169   225  2174   518]
 [  487 10844   738   891    97]
 [   57   684  2974    54   333]
 [  461   278    14 14949  1076]
 [   39    22    73   837  4641]]
TRAIN:  Pre. 0.797 | Rec. 0.806 | F1 0.798
[[2900  250   46  672  159]
 [ 653 1379  140 2884  445]
 [  61  123  247  222  622]
 [ 170  383   26 1056  176]
 [  17   40   49   81  215]]
EVAL:   Pre. 0.446 | Rec. 0.456 | F1 0.381 | BEST F1: 0.391 2

Epoch:    6 2022-09-03 22:32:53.861093
[[13139   928   189  1985   464]
 [  444 11143   674   723    73]
 [   67   658  3070    41   266]
 [  394   151     3 15251   979]
 [   32    13    44   826  4697]]
TRAIN:  Pre. 0.821 | Rec. 0.827 | F1 0.821
[[2930  206   45  689  157]
 [ 729 1345  150 2844  433]
 [  68  111  257  220  619]
 [ 167  345   20 1098  181]
 [  15   37   53   74  223]]
EVAL:   Pre. 0.452 | Rec. 0.467 | F1 0.386 | BEST F1: 0.391 2

Epoch:    7 2022-09-03 22:34:12.798169
[[12539  1729   270  1775   392]
 [  175 12005   673   189    15]
 [   35   656  3315     8    88]
 [  236   567    44 14875  1056]
 [   24    22   203   615  4748]]
TRAIN:  Pre. 0.821 | Rec. 0.842 | F1 0.827
[[2750  526  110  530  111]
 [ 513 2567  324 1809  288]
 [  54  194  500  118  409]
 [ 120  648   69  835  139]
 [  14   51  108   43  186]]
EVAL:   Pre. 0.461 | Rec. 0.493 | F1 0.453 | BEST F1: 0.391 2
[[5498 1299  257 1410  344]
 [ 563 4555  396 2159  258]
 [  67  337  968  125  537]
 [ 418 3216  266 4823  441]
 [  56  208  705  308 1207]]
TEST:   Pre. 0.532 | Rec. 0.537 | F1 0.529

Epoch:    8 2022-09-03 22:36:18.259686
[[13333  1149   223  1631   369]
 [  217 11997   661   168    14]
 [   39   562  3420     6    75]
 [  234   195     8 15334  1007]
 [   22    10    63   602  4915]]
TRAIN:  Pre. 0.848 | Rec. 0.868 | F1 0.856
[[2864  345   78  595  145]
 [ 558 1916  242 2398  387]
 [  54  136  399  142  544]
 [ 129  511   49  958  164]
 [  15   44   93   42  208]]
EVAL:   Pre. 0.456 | Rec. 0.484 | F1 0.425 | BEST F1: 0.453 7

Epoch:    9 2022-09-03 22:37:31.879097
[[13491  1232   219  1444   319]
 [  153 12116   695    83    10]
 [   25   453  3585     2    37]
 [  179   195    11 15354  1039]
 [   23     6    77   501  5005]]
TRAIN:  Pre. 0.857 | Rec. 0.883 | F1 0.867
[[2837  408   90  548  144]
 [ 554 2224  307 2086  330]
 [  55  133  478  110  499]
 [ 136  572   56  887  160]
 [  16   39  109   36  202]]
EVAL:   Pre. 0.460 | Rec. 0.495 | F1 0.443 | BEST F1: 0.453 7

Epoch:   10 2022-09-03 22:38:44.465717
[[13486  1073   196  1617   333]
 [  123 12079   729   117     9]
 [   23   382  3644     3    50]
 [  117    89     3 15571   998]
 [   12     4    41   481  5074]]
TRAIN:  Pre. 0.863 | Rec. 0.891 | F1 0.874
[[2702  332   74  733  186]
 [ 496 1666  211 2703  425]
 [  56   89  361  138  631]
 [ 108  467   40 1024  172]
 [   8   30   88   48  228]]
EVAL:   Pre. 0.454 | Rec. 0.478 | F1 0.407 | BEST F1: 0.453 7

Epoch:   11 2022-09-03 22:39:58.889015
[[14825   763   153   757   207]
 [  145 12104   737    66     5]
 [   22   341  3704     1    34]
 [  232   104     5 15368  1069]
 [   13     4    32   394  5169]]
TRAIN:  Pre. 0.881 | Rec. 0.911 | F1 0.894
[[3042  326   72  445  142]
 [ 813 1965  274 2082  367]
 [  98  107  435   99  536]
 [ 194  501   46  904  166]
 [  22   28   92   38  222]]
EVAL:   Pre. 0.457 | Rec. 0.501 | F1 0.437 | BEST F1: 0.453 7

Epoch:   12 2022-09-03 22:41:12.816113
[[15110   596   116   705   178]
 [  137 12144   699    70     7]
 [   24   342  3689     1    46]
 [  189    63     2 15545   979]
 [    9     4    21   410  5168]]
TRAIN:  Pre. 0.891 | Rec. 0.916 | F1 0.902
[[3005  236   54  564  168]
 [ 714 1463  178 2694  452]
 [  87   70  309  137  672]
 [ 174  400   37 1023  177]
 [  20   21   73   45  243]]
EVAL:   Pre. 0.453 | Rec. 0.485 | F1 0.401 | BEST F1: 0.453 7

Epoch:   13 2022-09-03 22:42:33.183972
[[15276   407    80   774   168]
 [  169 12071   685   123     9]
 [   29   297  3705     4    67]
 [  143    22     0 15706   907]
 [    8     0    10   413  5181]]
TRAIN:  Pre. 0.898 | Rec. 0.920 | F1 0.908
[[3009  193   50  605  170]
 [ 721 1180  142 2975  483]
 [  97   63  260  153  702]
 [ 175  302   29 1121  184]
 [  20   15   61   49  257]]
EVAL:   Pre. 0.455 | Rec. 0.485 | F1 0.385 | BEST F1: 0.453 7

Epoch:   14 2022-09-03 22:43:46.664200
[[15380   558   106   540   121]
 [  119 12276   605    52     5]
 [   21   323  3726     2    30]
 [  164    68     2 15619   925]
 [   10     1    23   355  5223]]
TRAIN:  Pre. 0.903 | Rec. 0.926 | F1 0.913
[[3051  421  100  343  112]
 [ 727 2411  335 1738  290]
 [  97  119  528   83  448]
 [ 191  607   67  805  141]
 [  20   34  120   32  196]]
EVAL:   Pre. 0.462 | Rec. 0.508 | F1 0.460 | BEST F1: 0.453 7
[[6165  988  272 1053  330]
 [ 909 4301  388 2054  279]
 [ 108  206 1030   80  610]
 [ 817 3023  266 4575  483]
 [ 137  110  712  181 1344]]
TEST:   Pre. 0.532 | Rec. 0.558 | F1 0.541

Epoch:   15 2022-09-03 22:45:48.545645
[[15592   323    67   577   146]
 [  151 12081   654   162     9]
 [   28   260  3750     7    57]
 [  145     2     0 15713   918]
 [    5     0     1   320  5286]]
TRAIN:  Pre. 0.906 | Rec. 0.930 | F1 0.917
[[3007  186   47  608  179]
 [ 722 1157  133 2983  506]
 [  97   59  251  139  729]
 [ 177  284   28 1136  186]
 [  18   15   59   48  262]]
EVAL:   Pre. 0.457 | Rec. 0.487 | F1 0.384 | BEST F1: 0.460 14

Epoch:   16 2022-09-03 22:47:04.550501
[[15528   396    85   581   115]
 [  107 12255   618    74     3]
 [   17   261  3792     4    28]
 [   97    19     0 15851   811]
 [    3     0     5   363  5241]]
TRAIN:  Pre. 0.912 | Rec. 0.934 | F1 0.922
[[2950  321   65  524  167]
 [ 673 1809  241 2376  402]
 [  87   75  408  126  579]
 [ 176  427   37  998  173]
 [  19   19   83   42  239]]
EVAL:   Pre. 0.465 | Rec. 0.505 | F1 0.432 | BEST F1: 0.460 14

Epoch:   17 2022-09-03 22:48:18.226832
[[15256   483    85   749   132]
 [   76 12241   684    54     2]
 [   13   201  3863     4    21]
 [   57    15     0 15853   853]
 [    3     0     9   281  5319]]
TRAIN:  Pre. 0.909 | Rec. 0.937 | F1 0.921
[[2803  353   75  617  179]
 [ 577 1703  228 2567  426]
 [  78   69  384  113  631]
 [ 146  446   39 1000  180]
 [  15   19   85   42  241]]
EVAL:   Pre. 0.456 | Rec. 0.492 | F1 0.416 | BEST F1: 0.460 14

Epoch:   18 2022-09-03 22:49:32.036013
[[15747   391    79   397    91]
 [   91 12257   647    58     4]
 [   13   198  3862     4    25]
 [   94     4     0 15879   801]
 [    3     0     6   284  5319]]
TRAIN:  Pre. 0.918 | Rec. 0.943 | F1 0.929
[[2978  332   77  484  156]
 [ 700 1900  270 2258  373]
 [  82   72  434  107  580]
 [ 166  447   41  979  178]
 [  18   17   86   42  239]]
EVAL:   Pre. 0.466 | Rec. 0.512 | F1 0.439 | BEST F1: 0.460 14

Epoch:   19 2022-09-03 22:50:47.005480
[[15838   350    59   380    78]
 [  104 12356   554    41     2]
 [   14   237  3829     1    21]
 [   92     8     3 15887   788]
 [    2     0     4   254  5352]]
TRAIN:  Pre. 0.924 | Rec. 0.946 | F1 0.934
[[3047  326   74  453  127]
 [ 740 1844  251 2297  369]
 [ 102   85  421  109  558]
 [ 181  417   37 1006  170]
 [  21   18   84   41  238]]
EVAL:   Pre. 0.467 | Rec. 0.514 | F1 0.440 | BEST F1: 0.460 14

Epoch:   20 2022-09-03 22:52:00.676075
[[15731   410    59   433    72]
 [   74 12505   462    15     1]
 [   14   265  3812     0    11]
 [   62    19     0 15988   709]
 [    2     1    11   258  5340]]
TRAIN:  Pre. 0.929 | Rec. 0.947 | F1 0.937
[[2899  383   75  514  156]
 [ 634 2086  263 2167  351]
 [  86  101  460  106  522]
 [ 157  487   41  956  170]
 [  17   24   91   36  234]]
EVAL:   Pre. 0.470 | Rec. 0.514 | F1 0.448 | BEST F1: 0.460 14

Epoch:   21 2022-09-03 22:53:18.209889
[[16028   293    45   291    48]
 [   83 12414   549    10     1]
 [    9   184  3900     0     9]
 [  100    18     0 15925   735]
 [    3     1    11   217  5380]]
TRAIN:  Pre. 0.931 | Rec. 0.954 | F1 0.942
[[3024  333   73  454  143]
 [ 733 1876  242 2266  384]
 [  94   82  437  106  556]
 [ 173  430   41  988  179]
 [  19   16   85   40  242]]
EVAL:   Pre. 0.470 | Rec. 0.516 | F1 0.443 | BEST F1: 0.460 14

Epoch:   22 2022-09-03 22:54:37.666119
[[15983   329    54   293    46]
 [   69 12433   543    11     1]
 [    9   170  3914     0     9]
 [   61    19     0 16074   624]
 [    2     1     6   257  5346]]
TRAIN:  Pre. 0.934 | Rec. 0.955 | F1 0.944
[[2991  345   75  478  138]
 [ 728 1944  254 2210  365]
 [  98   89  443  110  535]
 [ 171  448   41  984  167]
 [  19   18   87   45  233]]
EVAL:   Pre. 0.468 | Rec. 0.513 | F1 0.444 | BEST F1: 0.460 14

Epoch:   23 2022-09-03 22:55:50.446347
[[16002   283    45   326    49]
 [   73 12386   570    24     4]
 [    9   136  3941     1    15]
 [   58    11     0 15904   805]
 [    2     0     1   161  5448]]
TRAIN:  Pre. 0.931 | Rec. 0.957 | F1 0.943
[[2964  269   64  560  170]
 [ 664 1460  185 2719  473]
 [ 100   57  334  107  677]
 [ 164  305   31 1116  195]
 [  16   13   67   39  267]]
EVAL:   Pre. 0.468 | Rec. 0.509 | F1 0.414 | BEST F1: 0.460 14

Epoch:   24 2022-09-03 22:57:03.424270
[[16005   300    47   309    44]
 [   65 12442   538    11     1]
 [    7   137  3949     0     9]
 [   69    22     0 15990   697]
 [    2     1     4   177  5428]]
TRAIN:  Pre. 0.935 | Rec. 0.959 | F1 0.946
[[2914  306   73  559  175]
 [ 651 1784  241 2414  411]
 [  80   66  419  108  602]
 [ 156  426   41 1005  183]
 [  16   14   80   39  253]]
EVAL:   Pre. 0.468 | Rec. 0.512 | F1 0.433 | BEST F1: 0.460 14

Epoch:   25 2022-09-03 22:58:22.380032
[[16247   195    35   200    28]
 [   78 12473   492    13     1]
 [    8   148  3937     0     9]
 [  100    11     0 16004   663]
 [    6     1     8   168  5429]]
TRAIN:  Pre. 0.941 | Rec. 0.962 | F1 0.950
[[3092  296   77  425  137]
 [ 818 1844  250 2222  367]
 [ 114   79  428  100  554]
 [ 188  447   41  958  177]
 [  20   19   87   41  235]]
EVAL:   Pre. 0.464 | Rec. 0.510 | F1 0.438 | BEST F1: 0.460 14

Epoch:   26 2022-09-03 22:59:36.902957
[[16212   218    38   209    28]
 [   69 12462   517     8     1]
 [    3   132  3958     0     9]
 [   67    10     0 16061   640]
 [    5     0     1   174  5432]]
TRAIN:  Pre. 0.941 | Rec. 0.963 | F1 0.951
[[3018  319   83  466  141]
 [ 754 1940  279 2168  360]
 [ 103   72  461   95  544]
 [ 175  453   41  962  180]
 [  18   17   90   38  239]]
EVAL:   Pre. 0.468 | Rec. 0.518 | F1 0.446 | BEST F1: 0.460 14

Epoch:   27 2022-09-03 23:00:50.891582
[[16115   214    37   302    37]
 [   53 12506   477    20     1]
 [    3   127  3956     1    15]
 [   48    11     0 16115   604]
 [    2     0     1   173  5436]]
TRAIN:  Pre. 0.943 | Rec. 0.963 | F1 0.952
[[2924  293   71  577  162]
 [ 651 1623  196 2597  434]
 [  89   72  365  125  624]
 [ 164  383   35 1045  184]
 [  16   14   74   44  254]]
EVAL:   Pre. 0.465 | Rec. 0.503 | F1 0.420 | BEST F1: 0.460 14

Epoch:   28 2022-09-03 23:02:06.011207
[[16259   163    29   223    31]
 [   66 12553   422    15     1]
 [    4   147  3942     0     9]
 [   46     0     0 16172   560]
 [    2     0     0   188  5422]]
TRAIN:  Pre. 0.948 | Rec. 0.965 | F1 0.956
[[2999  255   63  559  151]
 [ 709 1351  155 2833  453]
 [ 102   66  305  131  671]
 [ 171  322   29 1107  182]
 [  17   17   65   46  257]]
EVAL:   Pre. 0.461 | Rec. 0.496 | F1 0.403 | BEST F1: 0.460 14

Epoch:   29 2022-09-03 23:03:18.647134
[[16222   184    35   235    29]
 [   66 12506   472    12     1]
 [    5   109  3979     0     9]
 [   44     1     0 16100   633]
 [    2     0     0   139  5471]]
TRAIN:  Pre. 0.945 | Rec. 0.967 | F1 0.955
[[2992  315   81  489  150]
 [ 697 1874  248 2293  389]
 [  96   74  432  105  568]
 [ 169  451   49  970  172]
 [  19   21   92   32  238]]
EVAL:   Pre. 0.465 | Rec. 0.510 | F1 0.438 | BEST F1: 0.460 14

Epoch:   30 2022-09-03 23:04:30.730818
[[16252   199    33   196    25]
 [   54 12569   421    12     1]
 [    3   118  3972     0     9]
 [   47     4     0 16146   581]
 [    2     0     5   149  5456]]
TRAIN:  Pre. 0.948 | Rec. 0.968 | F1 0.957
[[2992  367   98  439  131]
 [ 703 2137  292 2038  331]
 [ 102   87  487   96  503]
 [ 174  507   53  912  165]
 [  19   24   98   32  229]]
EVAL:   Pre. 0.467 | Rec. 0.517 | F1 0.453 | BEST F1: 0.460 14

Epoch:   31 2022-09-03 23:05:42.242578
[[16256   167    32   221    29]
 [   58 12531   456    11     1]
 [    2    94  3997     0     9]
 [   42     0     0 16117   619]
 [    2     0     0   118  5492]]
TRAIN:  Pre. 0.947 | Rec. 0.969 | F1 0.957
[[2948  267   69  576  167]
 [ 656 1494  189 2705  457]
 [  88   64  344  115  664]
 [ 167  355   37 1063  189]
 [  17   17   71   39  258]]
EVAL:   Pre. 0.462 | Rec. 0.500 | F1 0.412 | BEST F1: 0.460 14

Epoch:   32 2022-09-03 23:06:54.677433
[[16292   170    29   185    29]
 [   55 12532   456    13     1]
 [    2    85  4006     0     9]
 [   43     4     0 16093   638]
 [    1     0     4   105  5502]]
TRAIN:  Pre. 0.947 | Rec. 0.970 | F1 0.958
[[3000  311   81  487  148]
 [ 712 1887  263 2255  384]
 [  95   79  429   96  576]
 [ 174  473   50  937  177]
 [  18   23   92   29  240]]
EVAL:   Pre. 0.461 | Rec. 0.508 | F1 0.436 | BEST F1: 0.460 14

Epoch:   33 2022-09-03 23:08:08.220256
[[16288   170    27   191    29]
 [   54 12553   433    16     1]
 [    3    86  4003     0    10]
 [   39     1     0 16114   624]
 [    1     0     1    99  5511]]
TRAIN:  Pre. 0.949 | Rec. 0.971 | F1 0.959
[[2958  295   72  547  155]
 [ 682 1649  211 2520  439]
 [  91   68  380  112  624]
 [ 172  416   41 1000  182]
 [  17   19   80   36  250]]
EVAL:   Pre. 0.460 | Rec. 0.501 | F1 0.421 | BEST F1: 0.460 14

Epoch:   34 2022-09-03 23:09:20.581993
[[16263   182    28   205    27]
 [   52 12571   421    12     1]
 [    2    81  4010     0     9]
 [   27     0     0 16150   601]
 [    1     0     0   101  5510]]
TRAIN:  Pre. 0.950 | Rec. 0.972 | F1 0.960
[[2955  299   78  540  155]
 [ 656 1759  229 2437  420]
 [  89   73  404  105  604]
 [ 167  453   48  966  177]
 [  17   21   89   35  240]]
EVAL:   Pre. 0.460 | Rec. 0.500 | F1 0.426 | BEST F1: 0.460 14

Epoch:   35 2022-09-03 23:10:33.562365
[[16319   152    25   188    21]
 [   54 12604   387    11     1]
 [    2    92  3999     0     9]
 [   36     0     0 16191   551]
 [    1     0     0   116  5495]]
TRAIN:  Pre. 0.953 | Rec. 0.972 | F1 0.962
[[2977  295   69  538  148]
 [ 688 1724  211 2466  412]
 [ 100   76  385  112  602]
 [ 171  426   38  998  178]
 [  17   21   77   37  250]]
EVAL:   Pre. 0.465 | Rec. 0.506 | F1 0.427 | BEST F1: 0.460 14

Epoch:   36 2022-09-03 23:11:46.869457
[[16329   152    25   180    19]
 [   54 12603   389    10     1]
 [    2    89  4002     0     9]
 [   36     0     0 16204   538]
 [    2     0     0   117  5493]]
TRAIN:  Pre. 0.954 | Rec. 0.973 | F1 0.963
[[2974  293   70  541  149]
 [ 686 1695  210 2498  412]
 [  99   73  380  110  613]
 [ 170  431   37  991  182]
 [  17   20   78   37  250]]
EVAL:   Pre. 0.462 | Rec. 0.503 | F1 0.424 | BEST F1: 0.460 14

Epoch:   37 2022-09-03 23:13:00.378235
[[16341   130    23   191    20]
 [   56 12590   399    11     1]
 [    3    78  4012     0     9]
 [   32     0     0 16189   557]
 [    1     0     0   110  5501]]
TRAIN:  Pre. 0.954 | Rec. 0.973 | F1 0.963
[[2963  266   65  578  155]
 [ 671 1502  184 2698  446]
 [  93   63  342  120  657]
 [ 170  361   34 1063  183]
 [  17   15   73   41  256]]
EVAL:   Pre. 0.463 | Rec. 0.500 | F1 0.413 | BEST F1: 0.460 14

Epoch:   38 2022-09-03 23:14:13.835122
[[16359   130    23   175    18]
 [   56 12594   398     8     1]
 [    3    79  4011     0     9]
 [   40     0     0 16181   557]
 [    2     0     0   106  5504]]
TRAIN:  Pre. 0.954 | Rec. 0.973 | F1 0.963
[[2985  276   68  549  149]
 [ 703 1643  212 2525  418]
 [  99   69  368  112  627]
 [ 172  397   34 1025  183]
 [  17   20   75   37  253]]
EVAL:   Pre. 0.463 | Rec. 0.505 | F1 0.422 | BEST F1: 0.460 14

Epoch:   39 2022-09-03 23:15:28.546105
[[16357   132    23   175    18]
 [   56 12590   402     8     1]
 [    3    77  4013     0     9]
 [   40     0     0 16171   567]
 [    2     0     0   104  5506]]
TRAIN:  Pre. 0.953 | Rec. 0.973 | F1 0.963
[[2984  276   69  548  150]
 [ 703 1645  214 2520  419]
 [  98   68  372  109  628]
 [ 172  398   35 1023  183]
 [  17   20   75   37  253]]
EVAL:   Pre. 0.463 | Rec. 0.505 | F1 0.423 | BEST F1: 0.460 14

Some weights of the model checkpoint at ../model_base/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Inference:

[[15380   558   106   540   121]
 [  119 12276   605    52     5]
 [   21   323  3726     2    30]
 [  164    68     2 15619   925]
 [   10     1    23   355  5223]]
TRAIN:  Pre. 0.903 | Rec. 0.926 | F1 0.913
[[3051  421  100  343  112]
 [ 727 2411  335 1738  290]
 [  97  119  528   83  448]
 [ 191  607   67  805  141]
 [  20   34  120   32  196]]
EVAL:   Pre. 0.462 | Rec. 0.508 | F1 0.460
[[6165  988  272 1053  330]
 [ 909 4301  388 2054  279]
 [ 108  206 1030   80  610]
 [ 817 3023  266 4575  483]
 [ 137  110  712  181 1344]]
TEST:   Pre. 0.532 | Rec. 0.558 | F1 0.541

Process finished with exit code 0
